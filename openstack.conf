

#FIXME: Doesn't work. Need to specify these on the command line
#flags:
# ./pkb.py --cloud OpenStack --os_type ubuntu2004 --image Ubuntu-20.04 --benchmark_config_file openstack.conf
#  openstack_network: stackhpc-ipv4-geneve
#  os_type: ubuntu2004
#  cloud: OpenStack
#  image: Ubuntu-20.04

default_500_gb: &default_500_gb
 OpenStack:
    disk_size: 500
    disk_type: standard

local_disk: &local_disk
 OpenStack:
    disk_type: local
    device_path: /dev/sdb

hpl_vm: &hpl_vm
  OpenStack:
    zone: nova
    machine_type: m1.4xlarge
    image: none

benchmark_vm: &benchmark_vm
  OpenStack:
    zone: nova
    machine_type: benchmarking.whole-machine
    image: none

default_single_core: &default_single_core
  OpenStack:
    zone: nova
    machine_type: m1.small
    image: none

default_single_core: &default_dual_core
  OpenStack:
    zone: nova
    machine_type: m1.medium
    image: none

cassandra_stress: &cassandra_stress
  vm_groups:
    workers:
      vm_spec: *default_single_core
      disk_spec: *default_500_gb
      vm_count: 3
    client:
      vm_spec: *default_single_core

redis_memtier: &redis_memtier
  vm_groups:
    servers:
      vm_spec: *default_dual_core
      vm_count: 1
    clients:
      vm_spec: *default_dual_core
      vm_count: 1

unixbench: &unixbench
  vm_groups:
    default:
      vm_spec: *default_single_core
      disk_spec: *default_500_gb

cassandra_ycsb: &cassandra_ycsb
  vm_groups:
    workers:
      vm_spec: *default_single_core
      disk_spec: *default_500_gb
    clients:
      vm_spec: *default_single_core

mongodb_ycsb: &mongodb_ycsb
  vm_groups:
    workers:
      vm_spec: *default_single_core
      disk_spec: *default_500_gb
      vm_count: 1
    clients:
      vm_spec: *default_single_core

fio: &fio
  description: Runs fio in sequential, random, read and write modes.
  vm_groups:
    default:
      vm_spec: *benchmark_vm
      disk_spec: *default_500_gb
      vm_count: null
  flags:
    fio_target_mode: against_device_with_fill
    fio_generate_scenarios:
       - seq_1M_write_100%
       - seq_1M_read_100%
       - rand_4k_readwrite_100%
       - rand_4k_write_100%
       - rand_4k_read_100%

fio_local: &fio_local
  description: Runs fio in sequential, random, read and write modes (local disk).
  vm_groups:
    default:
      vm_spec: *benchmark_vm
      vm_count: null
      disk_spec: *local_disk
flags:
# user-data to disable mounting ephemeral disk
    openstack_post_provisioning_script: user-data-local-disk.txt
    fio_target_mode: against_device_with_fill
    fio_generate_scenarios:
       - seq_1M_write_100%
       - seq_1M_read_100%
       - rand_4k_readwrite_100%
       - rand_4k_write_100%
       - rand_4k_read_100%
    metadata: "disk:local"

iperf_tcp: &iperf_tcp
  description: Run iperf
  vm_groups:
    vm_1:
      vm_spec: *benchmark_vm
    vm_2:
      vm_spec: *benchmark_vm
  flags:
    openstack_scheduler_policy: anti-affinity
    iperf_runtime_in_seconds: 600
    iperf_sending_thread_count: 40
    iperf_benchmarks:
      - TCP

iperf_udp: &iperf_udp
  # NOTE: No parameter for UDP datagram size, so hard to get max
  # theoretical performance as the default of 1470 means we have
  # to transmit more packets than would be optimum.
  description: Run iperf
  vm_groups:
    vm_1:
      vm_spec: *benchmark_vm
    vm_2:
      vm_spec: *benchmark_vm
  flags:
    openstack_scheduler_policy: anti-affinity
    iperf_runtime_in_seconds: 600
    # Total bandwdith: sending_threads * bandwidth per stream
    # 40 * 1025M = 50G
    iperf_sending_thread_count: 40
    iperf_udp_per_stream_bandwidth: 1025
    iperf_benchmarks:
      - UDP

cluster_boot: &cluster_boot
  vm_groups:
    default:
      vm_spec: *default_single_core
      vm_count: 2
  flags:
    # We don't want boot time samples to be affected from retrying, so don't
    # retry cluster_boot when rate limited.
    retry_on_rate_limited: False

intel_hpl: &intel_hpl 
  vm_groups:
    default:
      vm_spec: *benchmark_vm
      vm_count: null
  flags:
    hpcc_use_intel_compiled_hpl: true
    hpcc_benchmarks: HPL
    metadata: "hpcc: intel"

hpcc: &hpcc
  vm_groups:
    default:
      vm_spec: *benchmark_vm
      vm_count: null
    
benchmarks:
  - iperf: *iperf_udp
  - iperf: * iperf_tcp
  - fio: *fio
  # Seems to require a secondary disk (ephemeral storage)
  - fio: *fio_local
  #- unixbench: *unixbench
  - hpcc: *hpcc
  # NOTE: you need to run deps.sh for this to work
  - hpcc: *intel_hpl
  #- cluster_boot: *cluster_boot
  #- redis_memtier: *redis_memtier
  # Disabled as fails to start cluster.
  #- cassandra_stress: *cassandra_stress

